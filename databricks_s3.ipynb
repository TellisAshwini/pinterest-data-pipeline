{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3abd155-f81e-42dc-9098-9ce208d26874",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4c68d20-8a64-47ab-9f10-b821fb1066e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae1d9a5c-9650-4657-aafc-20420304ca7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_type = \"csv\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577cbb01-5d31-4c8c-bde3-dfc2a3b8da72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "AWS_S3_BUCKET = \"user-12f7a43505b1-bucket\"\n",
    "MOUNT_NAME = \"/mnt/pinterest_s3_mount\"\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3526e23d-5a76-4ffb-8f8c-e5effb4bead5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"/mnt/pinterest_s3_mount/topics/12f7a43505b1.pin/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "df_pin = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "\n",
    "\n",
    "file_location = \"/mnt/pinterest_s3_mount/topics/12f7a43505b1.geo/partition=0/*.json\" \n",
    "df_geo = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "\n",
    "file_location = \"/mnt/pinterest_s3_mount/topics/12f7a43505b1.user/partition=0/*.json\" \n",
    "df_user = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdaedb20-0338-4913-ad64-8e8bfa46975c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_pin\n",
    "\"\"\"\n",
    "Replace empty entries and entries with no relevant data in each column with Nones\n",
    "\n",
    "\"\"\"\n",
    "df_pin = df_pin.dropDuplicates()\n",
    "print((df_pin.count(), len(df_pin.columns)))\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind')\n",
    "df_pin = df_pin.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \n",
    "\"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\", \"downloaded\")\n",
    "# df_pin.filter(df_pin['follower_count'].rlike('[A-Za-z]')).show()\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace('follower_count', '[%k]', '000'))\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace('follower_count', '[%M]', '000000'))\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace('follower_count', '[%User Info Error%]', ''))\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin['follower_count'].cast(IntegerType()))\n",
    "df_pin = df_pin.withColumn('save_location', regexp_replace('save_location', 'Local save in *', ''))\n",
    "#null_values = {col:df_pin.filter(df_pin[col].isNull()).count() for col in df_pin.columns}\n",
    "df_pin.na.fill('None', ['is_image_or_video', 'image_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e647fd28-a2a4-4b87-beff-c5ee3ff7bceb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_geo\n",
    "df_geo = df_geo.dropDuplicates()\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "df_geo = df_geo.withColumn(\"timestamp\", df_geo[\"timestamp\"].cast(TimestampType()))\n",
    "df_geo = df_geo.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "\n",
    "\n",
    "#df_user\n",
    "df_user = df_user.dropDuplicates()\n",
    "df_user = df_user.withColumn(\"user_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "df_user = df_user.withColumn('date_joined', df_user['date_joined'].cast(TimestampType()))\n",
    "df_user = df_user.select(\"ind\", \"user_name\", \"age\", \"date_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9418b007-7b35-4273-b952-d7fa622d1a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating temporary tables\n",
    "df_pin.createOrReplaceTempView('df_pin')\n",
    "df_geo.createOrReplaceTempView('df_geo')\n",
    "df_user.createOrReplaceTempView('df_user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b523cd2-baa1-4b1b-81e3-e9109be4bff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TASK 4: Most popular category in each country\n",
    "spark.sql(\"select country, category, category_count from (select *, count(category) over(partition by country) as category_count \\\n",
    "          from (select *, \\\n",
    "          dense_rank() over(partition by country order by rn desc) dr \\\n",
    "          from \\\n",
    "          (select country, category, max(rn) as rn from (select g.country, p.category, \\\n",
    "          count(p.category) over (partition by g.country order by g.country) as category_count, \\\n",
    "          row_number() over(partition by g.country, p.category order by g.country) rn\\\n",
    "          from df_geo g \\\n",
    "          inner join df_pin p \\\n",
    "          on g.ind = p.ind) X group by country, category)X) Y) Z where dr = 1\" ).show()\n",
    "\n",
    "#TASK 5: Most popular category in each year between  2018 and 2022\n",
    "#category_count is the number of time the highest category is repeated\n",
    "spark.sql(\"select post_year, category, m_rn as category_count from \\\n",
    "          (SELECT *, dense_rank() over (partition by post_year order by m_rn desc) DN from \\\n",
    "          (select post_year, category, max(rn) as m_rn from \\\n",
    "          (select *, row_number() over(partition by post_year, category order by post_year, category) rn from \\\n",
    "          (select EXTRACT(YEAR FROM g.timestamp) as post_year, p.category \\\n",
    "           from df_geo g inner join df_pin p on g.ind = p.ind \\\n",
    "          where EXTRACT(YEAR FROM g.timestamp) between 2018 and 2022 order by 1)X)Y group by post_year, category)Z)W \\\n",
    "          where DN = 1\").show()\n",
    "\n",
    "#TASK 6\n",
    "#STEP 1\n",
    "spark.sql(\"SELECT country, poster_name, follower_count from \\\n",
    "(select g.*,p.*, row_number() over (partition by country order by p.follower_count desc) rn \\\n",
    "from df_geo g inner join df_pin p on g.ind = p.ind) X \\\n",
    "where rn = 1 order by follower_count desc\").show()\n",
    "\n",
    "spark.sql(\"SELECT country, follower_count from \\\n",
    "(select g.*,p.*, row_number() over (partition by country order by p.follower_count desc) rn \\\n",
    "from df_geo g inner join df_pin p on g.ind = p.ind) X \\\n",
    "where rn = 1\").show()\n",
    "\n",
    "#TASK_7\n",
    "spark.sql(\"select age_group, category, rn as category_count from \\\n",
    "          (select *, dense_rank() over(partition by age_group order by rn desc) dn from \\\n",
    "          (select *, row_number() over(partition by age_group, category order by category) rn from \\\n",
    "          (select u.age, p.category, case \\\n",
    "          when u.age between 18 and 24 then '18-24' \\\n",
    "          when u.age between 25 and 35 then '25-35' \\\n",
    "          when u.age between 36 and 50 then '36-50' \\\n",
    "          when u.age > 50 then '+50' \\\n",
    "          end as age_group \\\n",
    "          from df_user u \\\n",
    "          inner join df_pin p \\\n",
    "          on u.ind = p.ind)X) Y)Z where dn = 1\").show()\n",
    "\n",
    "#TASK_8\n",
    "spark.sql(\"SELECT age_group, follower_count, rn, row_count from \\\n",
    "            (select *, row_number() over (partition by age_group order by follower_count) rn, \\\n",
    "            count(*) over(partition by age_group) row_count from \\\n",
    "                (select u.*, p.*, case \\\n",
    "                when u.age between 18 and 24 then '18-24' \\\n",
    "                when u.age between 25 and 35 then '25-35' \\\n",
    "                when u.age between 36 and 50 then '36-50' \\\n",
    "                when u.age > 50 then '+50' \\\n",
    "                end as age_group \\\n",
    "                from df_user u \\\n",
    "                inner join df_pin p \\\n",
    "                on u.ind = p.ind \\\n",
    "                ) X \\\n",
    "            )Y \\\n",
    "        where rn in ( FLOOR((row_count + 1) / 2), FLOOR( (row_count + 2) / 2) )\").show()\n",
    "\n",
    "#TASK_9\n",
    "spark.sql(\"select post_year, count(*) as number_users_joined from \\\n",
    "            (select *, EXTRACT(YEAR FROM date_joined) as post_year from df_user \\\n",
    "            )X \\\n",
    "          group by post_year\").show()\n",
    "\n",
    "#TASK_10\n",
    "spark.sql(\"select post_year, rn median_follower_count from (select post_year, row_number() over(partition by post_year order by post_year) rn, \\\n",
    "          count(*) over(Partition by post_year) u_count from \\\n",
    "            (select *, EXTRACT(YEAR FROM date_joined) as post_year from df_user)X) Y \\\n",
    "            where rn in ( FLOOR((u_count + 1) / 2), FLOOR( (u_count + 2) / 2) )\").show()\n",
    "\n",
    "#TASK_11\n",
    "spark.sql(\"select age_group, post_year, follower_count as median_follower_count from (select *, \\\n",
    "          row_number() over(partition by post_year, age_group order by post_year) rn, \\\n",
    "          count(*) over(partition by post_year, age_group) row_count from \\\n",
    "          (select p.follower_count, u.age, \\\n",
    "          case \\\n",
    "          when u.age between 18 and 24 then '18-24' \\\n",
    "          when u.age between 25 and 35 then '25-35' \\\n",
    "          when u.age between 36 and 50 then '36-50' \\\n",
    "          when u.age > 50 then '+50' \\\n",
    "          end as age_group, \\\n",
    "          EXTRACT(YEAR FROM g.timestamp) as post_year \\\n",
    "           from df_user u \\\n",
    "          inner join df_pin p on u.ind = p.ind \\\n",
    "          inner join df_geo g on p.ind = g.ind) X \\\n",
    "          where post_year between 2015 and 2020 \\\n",
    "          order by post_year)Y \\\n",
    "          where rn in ( FLOOR((row_count + 1) / 2), FLOOR( (row_count + 2) / 2) ) \").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4247869288787811,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_s3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

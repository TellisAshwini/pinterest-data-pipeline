{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Streaming Data Processing using Spark on Databricks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0acad73-dced-4ba6-93f7-84061953cc4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import urllib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read the csv file containing the AWS keys to the databricks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f5b2687-39ac-490a-877c-2f84d09a6429",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_type = \"csv\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "KINESIS_REGION = \"us-east-1\"\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Loads the streaming data into the dataframes (`df_pin`, `df_geo` and `df_user`), from the kinesis streams*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67de81db-af74-4780-bc89-42bc8758ea1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kinesis_dict = {'df_pin_stream': {'kinesis_pin':\"streaming-12f7a43505b1-pin\"}, 'df_geo_stream':{'kinesis_geo':\"streaming-12f7a43505b1-geo\"}, 'df_user_stream':{'kinesis_user':\"streaming-12f7a43505b1-user\"}}\n",
    "for key, value in kinesis_dict.items():\n",
    "    for stream_name, kinesis_streaming_name in value.items():\n",
    "        KINESIS_STREAM_NAME = f\"{kinesis_streaming_name}\"\n",
    "        stream_name = (spark.readStream\n",
    "        .format(\"kinesis\")\n",
    "        .option(\"streamName\", KINESIS_STREAM_NAME)\n",
    "        .option(\"region\", KINESIS_REGION)\n",
    "        .option(\"initialPosition\", '{\"at_timestamp\": \"05/23/2023 00:00:00 GMT\", \"format\": \"MM/dd/yyyy HH:mm:ss ZZZ\"}')\n",
    "        .option(\"awsAccessKey\", ACCESS_KEY)\n",
    "        .option(\"awsSecretKey\", ENCODED_SECRET_KEY)\n",
    "        .load())\n",
    "    key = stream_name.selectExpr(\"cast (data as STRING) jsonData\", \"approximateArrivalTimestamp\").withColumn(\"approximateArrivalDate\", to_date(col(\"approximateArrivalTimestamp\")))  \n",
    "\n",
    "\n",
    "df_pin = df_pin_stream.select(json_tuple('jsonData', \"index\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\",  \"save_location\", \"category\", \"downloaded\").alias(\"index\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\",  \"save_location\", \"category\", \"downloaded\"))\n",
    "df_geo = df_geo_stream.select(json_tuple('jsonData', 'ind', 'country', 'latitude', 'longitude', 'timestamp').alias('ind', 'country', 'latitude', 'longitude', 'timestamp'))\n",
    "df_user = df_user_stream.select(json_tuple('jsonData', 'ind', \"first_name\", \"last_name\", 'age', 'date_joined').alias('ind', \"first_name\", \"last_name\", 'age', 'date_joined'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data cleaning for df_pin*\n",
    " - Removing duplicate rows in the dataframe\n",
    " - Renaming the column `index` to `ind`\n",
    " - Re-ordering the column names in the dataframe\n",
    " - Replacing the values of `follower_count` column wherever necessary.\n",
    " - Converting the columns `follower_count`, `ind`, `downloaded` into a integer data type\n",
    " - Removing any additional strings from the `save_location` column\n",
    " - Replacing all the NA with `None`\n",
    " - Dropping the rows where all columns have null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pin = df_pin.dropDuplicates()\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind')\n",
    "df_pin = df_pin.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \n",
    "\"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\", \"downloaded\")\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace('follower_count', '[%k]', '000'))\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace('follower_count', '[%M]', '000000'))\n",
    "df_pin = df_pin.withColumn('follower_count', regexp_replace('follower_count', '[%User Info Error%]', ''))\n",
    "df_pin = df_pin.withColumn('ind', df_pin['ind'].cast(IntegerType()))\n",
    "df_pin = df_pin.withColumn('downloaded', df_pin['downloaded'].cast(IntegerType()))\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin['follower_count'].cast(IntegerType()))\n",
    "df_pin = df_pin.withColumn('save_location', regexp_replace('save_location', 'Local save in *', ''))\n",
    "df_pin.na.fill('None', ['is_image_or_video', 'image_src'])\n",
    "df_pin.na.drop(how = \"all\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data cleaning for df_geo*\n",
    " - Removing duplicate rows in the dataframe\n",
    " - Converting the columns `ind` to integer type, `latitude` and `longitude` to double type, `timestamp` into a timestamp data type\n",
    " - Creating new column `coordinates` with the values to be the array of `latitude` and `longitude` column and deleting these two columns\n",
    " - Re-ordering the column names in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = df_geo.dropDuplicates()\n",
    "df_geo = df_geo.withColumn('longitude', df_geo['longitude'].cast(DoubleType()))\n",
    "df_geo = df_geo.withColumn('ind', df_geo['ind'].cast(IntegerType()))\n",
    "df_geo = df_geo.withColumn(\"timestamp\", df_geo[\"timestamp\"].cast(TimestampType()))\n",
    "\n",
    "df_geo = df_geo.withColumn('latitude', df_geo['latitude'].cast(DoubleType()))\n",
    "df_geo = df_geo.withColumn('longitude', df_geo['longitude'].cast(DoubleType()))\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "df_geo = df_geo.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Data cleaning for df_user*\n",
    " - Removing duplicate rows in the dataframe\n",
    " - Creating new column `user_name` by combining the `first_name` and `last_name` column and deleting these two columns\n",
    " - Converting the `date_joined` column into a timestamp data type and `age` to integer data type\n",
    " - Re-ordering the column names in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "543d1f92-80ea-4a37-8121-a3f0203d71f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_user = df_user.dropDuplicates()\n",
    "df_user = df_user.withColumn(\"user_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "df_user = df_user.withColumn('date_joined', df_user['date_joined'].cast(TimestampType()))\n",
    "df_user = df_user.withColumn('age', df_user['age'].cast(TimestampType()))\n",
    "df_user = df_user.select(\"ind\", \"user_name\", \"age\", \"date_joined\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Creating three delta tables for the three kinesis data streams:* \n",
    "    - *`12f7a43505b1_pin_table`*\n",
    "    - *`12f7a43505b1_geo_table`*\n",
    "    - *`12f7a43505b1_user_table`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aad08d19-9624-4d67-a52e-0138ded13d56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables = {df_pin :'12f7a43505b1_pin_table', df_geo :'12f7a43505b1_geo_table', df_user :'12f7a43505b1_user_table'}\n",
    "for df_name, delta_table in tables:\n",
    "    df_name.writeStream.format(\"delta\").option(\"checkpointLocation\",f\"dbfs:/user/hive/warehouse/{delta_table}\").table(f'{delta_table}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 621810419625197,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_kinesis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
